# config.py - GPUä¼˜åŒ–ç‰ˆæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
import torch

class Config:
    # ========== GPUä¼˜åŒ–é…ç½® ==========
    # è®¾å¤‡é…ç½® - å¼ºåˆ¶ä½¿ç”¨GPU
    USE_GPU = True  # å¼ºåˆ¶ä½¿ç”¨GPU
    FORCE_CPU = False  # å¼ºåˆ¶ä½¿ç”¨CPUï¼ˆè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
    
    if FORCE_CPU:
        DEVICE = torch.device('cpu')
        print("å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    elif USE_GPU and torch.cuda.is_available():
        DEVICE = torch.device('cuda:0')
        # è®¾ç½®CUDAä¼˜åŒ–æ ‡å¿—
        torch.backends.cudnn.benchmark = True  # åŠ é€Ÿå·ç§¯è¿ç®—
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.deterministic = False  # ä¸ºäº†é€Ÿåº¦ç‰ºç‰²ä¸€ç‚¹ç¡®å®šæ€§
        
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ¯ ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name}")
        print(f"  å†…å­˜: {gpu_memory:.2f} GB")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
    else:
        DEVICE = torch.device('cpu')
        if USE_GPU:
            print("âš  GPUä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
        else:
            print("â„¹ ä½¿ç”¨CPUæ¨¡å¼")
    
    # ========== æ•°æ®é…ç½® ==========
    DATA_PATH = "data/CICIDS2017"
    SAMPLE_FRACTION = 0.3  # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    TEST_SIZE = 0.3
    VAL_SIZE = 0.2
    
    # ========== CNNæ¨¡å‹é…ç½®ï¼ˆGPUä¼˜åŒ–ï¼‰ ==========
    CNN_CONFIG = {
        'hidden_channels': [64, 128, 256],  # å·ç§¯å±‚é€šé“æ•°
        'kernel_sizes': [3, 3, 3],  # å·ç§¯æ ¸å¤§å°
        'pool_sizes': [2, 2, 2],  # æ± åŒ–å¤§å°
        'fc_sizes': [256, 128],  # å¢åŠ å…¨è¿æ¥å±‚å¤§å°ï¼ˆGPUæœ‰è¶³å¤Ÿå†…å­˜ï¼‰
        'dropout_rate': 0.3,
        'batch_norm': True,
        'activation': 'relu'  # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
    }
    
    # ========== è®­ç»ƒé…ç½®ï¼ˆGPUä¼˜åŒ–ï¼‰ ==========
    TRAIN_CONFIG = {
        'batch_size': 512 if DEVICE.type == 'cuda' else 64,  # GPUç”¨å¤§batch
        'learning_rate': 0.001,
        'epochs': 100,
        'patience': 10,
        'weight_decay': 1e-4,
        'gradient_accumulation_steps': 1,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        'mixed_precision': True if DEVICE.type == 'cuda' else False,  # GPUç”¨æ··åˆç²¾åº¦
        'pin_memory': True if DEVICE.type == 'cuda' else False,  # GPUå›ºå®šå†…å­˜
        'num_workers': 4 if DEVICE.type == 'cuda' else 2,  # GPUç”¨æ›´å¤šworkers
        'persistent_workers': True,  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
        'prefetch_factor': 2,  # é¢„å–å› å­
        'scheduler_type': 'plateau',  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
        'warmup_epochs': 3,  # å­¦ä¹ ç‡é¢„çƒ­è½®æ•°
        'max_grad_norm': 1.0,  # æ¢¯åº¦è£å‰ª
    }
    
    # ========== FPA-COAæ··åˆä¼˜åŒ–é…ç½® ==========
    OPTIMIZER_CONFIG = {
        'pop_size': 30,
        'iter_max': 20,
        'fpa_params': {
            'p': 0.8,      # è½¬æ¢æ¦‚ç‡
            'lambda_': 1.5, # LÃ©vyé£è¡Œå‚æ•°
            'alpha': 0.1,   # æ­¥é•¿ç¼©æ”¾å› å­
            # åˆ é™¤ 'use_gpu': True  # FPAä¸æ”¯æŒæ­¤å‚æ•°
        },
        'coa_params': {
            'pa': 0.25,    # å®¿ä¸»å‘ç°æ¦‚ç‡
            'alpha': 0.01, # æ­¥é•¿å› å­
            'beta': 1.5,   # å¹‚å¾‹åˆ†å¸ƒå‚æ•°
            # åˆ é™¤ 'use_gpu': True  # COAä¸æ”¯æŒæ­¤å‚æ•°
        },
        'hybrid_params': {
            'elite_rate': 0.1,     # ç²¾è‹±ä¿ç•™æ¯”ä¾‹
            'migration_rate': 0.2, # ç§ç¾¤è¿ç§»æ¯”ä¾‹
            'adaptive_weight': True, # è‡ªé€‚åº”æƒé‡
            'collaboration_frequency': 5,
            'gpu_accelerated': True if DEVICE.type == 'cuda' else False,  # GPUåŠ é€Ÿæ ‡å¿—
        }
    }
    
    # ========== ç‰¹å¾é€‰æ‹©é…ç½®ï¼ˆGPUä¼˜åŒ–ï¼‰ ==========
    FEATURE_SELECTION = {
        'threshold': 0.5,
        'min_features': 10,
        'max_features_ratio': 0.8,
        'fast_eval': True,  # å¿«é€Ÿè¯„ä¼°æ¨¡å¼
        'eval_batch_size': 256 if DEVICE.type == 'cuda' else 64,  # GPUç”¨å¤§batch
        'eval_epochs': 3,  # è¯„ä¼°æ—¶è®­ç»ƒè½®æ•°
        'use_gpu_for_eval': True if DEVICE.type == 'cuda' else False,  # è¯„ä¼°æ—¶ä½¿ç”¨GPU
    }
    
    # ========== GPUå†…å­˜ç®¡ç†é…ç½® ==========
    GPU_CONFIG = {
        'empty_cache_frequency': 50,  # æ¯50ä¸ªbatchæ¸…ç†ä¸€æ¬¡ç¼“å­˜
        'memory_monitor': True,  # ç›‘æ§GPUå†…å­˜ä½¿ç”¨
        'max_memory_usage': 0.8,  # æœ€å¤§GPUå†…å­˜ä½¿ç”¨ç‡ï¼ˆ80%ï¼‰
    }
    
    # ========== æ—¥å¿—å’Œä¿å­˜é…ç½® ==========
    LOG_CONFIG = {
        'log_interval': 10,  # æ—¥å¿—é—´éš”
        'save_checkpoints': True,  # ä¿å­˜æ£€æŸ¥ç‚¹
        'checkpoint_frequency': 5,  # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆepochï¼‰
        'experiment_name': 'fpa_coa_cnn_gpu',  # å®éªŒåç§°
    }
    
    # ========== æ€§èƒ½ä¼˜åŒ–é…ç½® ==========
    PERFORMANCE_CONFIG = {
        'use_amp': True if DEVICE.type == 'cuda' else False,  # GPUç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        'channels_last': True if DEVICE.type == 'cuda' else False,  # GPUç”¨channels_lastæ ¼å¼
    }
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self._setup_directories()
        self._print_config_summary()
        self.optimize_for_gpu()
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        import os
        
        # åŸºç¡€ç›®å½•
        os.makedirs("model", exist_ok=True)
        os.makedirs("results", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        # GPUä¸“ç”¨ç›®å½•
        if self.DEVICE.type == 'cuda':
            os.makedirs("model/gpu_models", exist_ok=True)
            os.makedirs("results/gpu_results", exist_ok=True)
            os.makedirs("logs/gpu_logs", exist_ok=True)
    
    def _print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("é…ç½®æ‘˜è¦")
        print("="*60)
        print(f"è®¾å¤‡: {self.DEVICE}")
        
        if self.DEVICE.type == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"æ‰¹é‡å¤§å°: {self.TRAIN_CONFIG['batch_size']}")
            print(f"æ··åˆç²¾åº¦: {self.TRAIN_CONFIG['mixed_precision']}")
            print(f"æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹: {self.TRAIN_CONFIG['num_workers']}")
            print(f"ç‰¹å¾é€‰æ‹©è¯„ä¼°batch: {self.FEATURE_SELECTION['eval_batch_size']}")
        else:
            print(f"æ‰¹é‡å¤§å°: {self.TRAIN_CONFIG['batch_size']}")
            print(f"ç‰¹å¾é€‰æ‹©è¯„ä¼°batch: {self.FEATURE_SELECTION['eval_batch_size']}")
        
        print(f"CNNå±‚æ•°: {len(self.CNN_CONFIG['hidden_channels'])}")
        print(f"FPA-COAç§ç¾¤å¤§å°: {self.OPTIMIZER_CONFIG['pop_size']}")
        print(f"ç‰¹å¾é€‰æ‹©æœ€å°ç‰¹å¾æ•°: {self.FEATURE_SELECTION['min_features']}")
        print(f"æ•°æ®é‡‡æ ·æ¯”ä¾‹: {self.SAMPLE_FRACTION}")
        print("="*60 + "\n")
    
    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if self.DEVICE.type != 'cuda':
            return None
        
        try:
            info = {
                'total': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                'allocated': torch.cuda.memory_allocated() / 1024**3,
                'reserved': torch.cuda.memory_reserved() / 1024**3,
                'free': (torch.cuda.get_device_properties(0).total_memory - 
                        torch.cuda.memory_allocated()) / 1024**3
            }
            return info
        except:
            return None
    
    def optimize_for_gpu(self):
        """åº”ç”¨GPUä¼˜åŒ–"""
        if self.DEVICE.type != 'cuda':
            return
        
        try:
            # è®¾ç½®Tensoræ ¸å¿ƒä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(torch.cuda, 'set_float32_matmul_precision'):
                torch.cuda.set_float32_matmul_precision('high')
            
            # å¯ç”¨TF32ï¼ˆå¦‚æœGPUæ”¯æŒï¼‰
            if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            print("âœ… GPUä¼˜åŒ–å·²åº”ç”¨")
        except Exception as e:
            print(f"âš  GPUä¼˜åŒ–åº”ç”¨å¤±è´¥: {e}")
    
    def print_current_gpu_status(self):
        """æ‰“å°å½“å‰GPUçŠ¶æ€"""
        if self.DEVICE.type == 'cuda':
            memory_info = self.get_gpu_memory_info()
            if memory_info:
                print(f"\nğŸ“Š å½“å‰GPUçŠ¶æ€:")
                print(f"  å·²åˆ†é…å†…å­˜: {memory_info['allocated']:.2f} GB")
                print(f"  å·²ç¼“å­˜å†…å­˜: {memory_info['reserved']:.2f} GB")
                print(f"  å¯ç”¨å†…å­˜: {memory_info['free']:.2f} GB")
                print(f"  æ€»å†…å­˜: {memory_info['total']:.2f} GB")
    
    def get_training_params(self):
        """è·å–è®­ç»ƒå‚æ•°"""
        return {
            'device': str(self.DEVICE),
            'batch_size': self.TRAIN_CONFIG['batch_size'],
            'learning_rate': self.TRAIN_CONFIG['learning_rate'],
            'epochs': self.TRAIN_CONFIG['epochs'],
            'mixed_precision': self.TRAIN_CONFIG['mixed_precision'],
            'num_workers': self.TRAIN_CONFIG['num_workers'],
            'use_gpu': self.DEVICE.type == 'cuda',
            'gpu_name': torch.cuda.get_device_name(0) if self.DEVICE.type == 'cuda' else 'CPU'
        }

# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
config = Config()

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("é…ç½®æµ‹è¯•å®Œæˆ!")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    params = config.get_training_params()
    print(f"\nè®­ç»ƒå‚æ•°:")
    for key, value in params.items():
        print(f"  {key}: {value}")
    
    # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
    config.print_current_gpu_status()